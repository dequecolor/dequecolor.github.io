---
title: "决策树"
date: 2019-12-03T16:54:13
categories:
  - 机器学习算法
tags:
  - 决策树
  - 机器学习
toc: true
---

决策树（decision tree）是一种常见的机器学习方法，是一种基本的分类和回归方法。本文主要讨论用于分类的决策树。  

决策树是树形的，在分类问题中，表示的是基于特征对实例进行分类的过程，可以认为是`if-then`规则的集合，
也可以认为是定义在特征空间与类空间上的条件概率分布。主要优点是可读性高、分类速度快。在学习阶段，利用训练数据，
根据损失函数最小化原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类。

决策树学习通常包含3个步骤：
- 特征选择
- 决策树的生成
- 决策树的修剪

决策树的思想主要来源于Quinlan在1986年提出的ID3算法和1993年提出的C4.5算法，以及由Breiman等人在1984年提出的CART算法。

本文会首先介绍决策树的基本概念，然后通过ID3和C4.5介绍特征选择、决策树的生成以及决策树的修剪，最后介绍CART算法。  

## 决策树模型与学习
### 决策树模型

**定义**  分类决策树模型是一种描述对实例进行分类的树形结构。决策树由节点（node）和有向边（directed edge）组成。节点有两种类型：
内部节点（internal node）和叶节点（leaf node）。内部节点表示一个特征或属性，叶节点表示一个类。

用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到子节点；这时，每一个子节点对应着该特征的一个取值。
如此递归地对实例进行测试与分类，直至达到叶节点，最后将实例分到叶节点的类中。

### 决策树与`if-then`规则

可以将决策树看成一个`if-then`规则的集合。将决策树转换成`if-then`规则的过程是这样的：由决策树的根节点到叶节点的每一条路径构建一条规则；
路径上的内部节点的特征对应着规则的条件，而叶节点的类对应着规则的结论。决策树的路径或其对应的`if-then`规则集合有一个重要的性质：互斥并且完备。
这就是说，每个实例都能被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。这里所谓覆盖是指实例的特征与路径上的特征一致或实例满足
规则的条件。

### 决策树与条件概率分布

决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分上。将特征空间划分为互不相交的单元或区域。并在每个单元
定义一个类的概率分布就构成了一个条件概率分布。决策树的一条路径对应于划分中的一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条
件概率分布组成。假设X是表示特征的随机变量，Y为表示类的随机变量，那么这个条件概率分布可以表示为P(Y|X)，X取值于给定划分下单元的集合，Y取值
于类的集合。各叶节点（单元）上的条件概率往往偏向于某个类，即属于某个类的概率较大。决策树分类时将该节点的实例强行分到条件概率大的那一类去。

### 决策树学习

假设给定训练集

## 特征选择
### 特征选择问题
### 信息增益
### 信息增益比
### ID3算法
### C4.5的生成算法

## 决策树的剪枝

## CART算法
### CART生成
### CART剪枝

## 本文概要

参考李航的《统计学习方法》
