---
title: "决策树"
date: 2019-12-03T16:54:13
categories:
  - 机器学习算法
tags:
  - 决策树
  - 机器学习
toc: true
<!--toc_sticky: true-->
---


决策树（decision tree）是一种常见的机器学习方法，是一种基本的分类和回归方法。本文主要讨论用于分类的决策树。  
决策树是树形的，在分类问题中，表示的是基于特征对实例进行分类的过程，可以认为是`if-then`规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。主要优点是可读性高、分类速度快。在学习阶段，利用训练数据，根据损失函数最小化原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类。   
决策树学习通常包含3个步骤：
- 特征选择
- 决策树的生成
- 决策树的修剪   

决策树的思想主要来源于Quinlan在1986年提出的ID3算法和1993年提出的C4.5算法，以及由Breiman等人在1984年提出的CART算法。  
本文会首先介绍决策树的基本概念，然后通过ID3和C4.5介绍特征选择、决策树的生成以及决策树的修剪，最后介绍CART算法。  
## 决策树模型与学习
### 决策树模型  
**定义**  分类决策树模型是一种描述对实例进行分类的树形结构。决策树由节点（node）和有向边（directed edge）组成。节点有两种类型：内部节点（internal node）和叶节点（leaf node）。内部节点表示一个特征或属性，叶节点表示一个类。  

![决策树][1]  

用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到子节点；这时，每一个子节点对应着该特征的一个取值。如此递归地对实例进行测试与分类，直至达到叶节点，最后将实例分到叶节点的类中。  
### 决策树与`if-then`规则  
可以将决策树看成一个`if-then`规则的集合。将决策树转换成`if-then`规则的过程是这样的：由决策树的根节点到叶节点的每一条路径构建一条规则；路径上的内部节点的特征对应着规则的条件，而叶节点的类对应着规则的结论。决策树的路径或其对应的`if-then`规则集合有一个重要的性质：互斥并且完备。这就是说，每个实例都能被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。这里所谓覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件。  
### 决策树与条件概率分布  
决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分上。将特征空间划分为互不相交的单元或区域。并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的一条路径对应于划分中的一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。假设X是表示特征的随机变量，Y为表示类的随机变量，那么这个条件概率分布可以表示为$$P=(Y \vert X)$$，X取值于给定划分下单元的集合，Y取值于类的集合。各叶节点（单元）上的条件概率往往偏向于某个类，即属于某个类的概率较大。决策树分类时将该节点的实例强行分到条件概率大的那一类去。  
### 决策树学习  
假设给定训练集  

$$ D = \{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$$  

其中，$$x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})$$为输入实例（特征向量），n为特征个数，$$y_i \in \{1,2,...K\}$$为类标记，$$i=1,2,...,N$$，N为样本容量。学习的目标是根据给定的训练数据集构建一个决策树模型，使他能够对实例进行正确的分类。  
决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树（即是能够对训练数据进行正确分类的决策树）可能有多个，也可能一个也没有。我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。从另一个角度看，决策树学习是由训练数据集估计条件概率模型，基于特征空间划分的类的条件概率模型有无穷多个。我们选择的条件概率模型应该不仅对训练数据有很好的拟合，而且对未知数据有很好的预测。  
决策树学习用损失函数表示这一目标。如下所述，决策树学习的损失函数通常是正则化的极大似然函数，决策树学习的策略是以损失函数为目标函数的最小化。  
当损失函数确定以后，学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题。这样得到的决策树是次最优的。  
决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。开始，构建根节点，将所有数据都放在根节点。选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集基本能够被正确分类，那么构建叶节点，并将这些子集分到所对应的叶节点中去，如果还有不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的节点，如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶节点上，即有了明确的类，这就生成了一颗决策树。  
以上方法生成的决策树可能对训练数据有很好的分类能力，但对未知的测试数据却未必有很好的分类能力，即可能发生过拟合现象。我们需要对已生成的树自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。具体地，就是去掉过于细分的叶节点，使其回退到父节点，甚至更高的节点，然后将父节点或者更高的节点改为新的叶节点。  
如果特征数量很多，也可以在决策树学习开始的时候，对特征进行选择，只留下对训练数据有足够分类能力的特征。  
可以看出，决策树学习算法包含特征选择、决策树的生成与决策树的剪枝过程。由于决策树表示一个条件概率分布，所以深浅不同的决策树对应着不同复杂度的概率模型，决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优。  
决策树学习常用的算法有**ID3**、**C4.5**与**CART**，下面结合这些算法分别叙述决策树学习的特征选择、决策树的生成和剪枝过程。  
## 特征选择
### 特征选择问题  
特征选择在于选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的准则是信息增益或信息增益比。  
首先通过一个例子来说明特征选择问题。  
下表是一个由15个样本组成的贷款申请训练数据。数据包括贷款申请人的4个特征（属性）：第一个特征是年龄，有三个可能值：青年，中年，老年；第2个特征是有工作，有2个可能值：是，否；第3个特征是有自己的房子，有2个可能值：是，否；第4个特征是信贷情况，有3个可能值：非常好，好，一般。表的最后一列是类别，是否同意贷款，取2个值：是，否。

| ID | 年龄 | 有工作 | 有自己的房子 |  信贷情况 | 类别 |
| --- | --- | --- | --- | --- | --- |
| 1 | 青年 | 否 | 否 | 一般 | 否 |
| 2 | 青年 | 否 | 否 | 好 | 否 |
| 3 | 青年 | 是 | 否 | 好 | 是 |
| 4 | 青年 | 是 | 是 | 一般 | 是 |
| 5 | 青年 | 否 | 否 | 一般 | 否 |
| 6 | 中年 | 否 | 否 | 一般 | 否 |
| 7 | 中年 | 否 | 否 | 好 | 否 |
| 8 | 中年 | 是 | 是 | 好 | 是 |
| 9 | 中年 | 否 | 是 | 非常好 | 是 |
| 10 | 中年 | 否 | 是 | 非常好 | 是 |
| 11 | 老年 | 否 | 是 | 非常好 | 是 |
| 12 | 老年 | 否 | 是 | 好 | 是 |
| 13 | 老年 | 是 | 否 | 好 | 是 |
| 14 | 老年 | 是 | 否 | 非常好 | 是 |
| 15 | 老年 | 否 | 否 | 一般 | 否 |  

希望通过所给的训练数据学习一个贷款申请的决策树，用以对未来的贷款申请进行分类，即当新的客户提出贷款申请时，根据申请人的特征利用决策树决定是否批准贷款申请。  
特征选择是决定用哪个特征来划分特征空间。  
下图是从表的数据学习到的两个可能的决策树，分别由两个不同特征的根节点构成。（a）所示的根节点的特征是年龄，有3个取值，对应于不同的取值有不同的子节点。（b）所示的根节点的特征是有工作，有2个取值，对应于不同的取值有不同的子节点。两个决策树都可以从此延续下去。问题是：究竟选择哪一个特征更好些？这就要求确定选择特征的准则。直观上，如果一个特征具有更好的分类能力，或者说，按照这个特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。信息增益就能够很好地表示这一直观的准则。

![可能学习到的决策树][2]

### 信息增益  
为了便于说明，先给出熵和条件熵的定义。  
在信息论和概率统计中，熵（entropy）是表示随机变量不确定性的度量，设X是一个取有限个值的离散随机变量，其概率分布为  

$$P(X=x_i)=p_i, i=1,2,...,n$$  

则随机变量X的熵定义为  

$$H(X)=-\sum_{i=1}^n{p_i\log p_i}$$  

在上式中，若$$p_i=0$$，则定义$$0\log 0=0$$，通常式中的对数以2为底或以e（自然对数）为底，这时熵的单位分别称作比特(bit)或纳特(nat)。由定义可知，熵只依赖于X的分布，而与X的取值无关，所以也可以将X的熵记作$$H(p)$$，即  

$$H(p)=-\sum_{i=1}^n{p_i\log p_i}$$  

熵越大，随机变量的不确定性就越大，从定义可验证  

$$0\leq H(p)\leq \log n$$  

当随机变量只取两个值，例如1,0时，即X的分布为  

$$P(X=1)=p,   P(X=0)=1-p,   0\leq p\leq 1$$  

熵为  

$$H(p)=-p\log_2 p-(1-p)\log_2 (1-p)$$  

这时，熵$$H(p)$$随概率p变化的曲线如下图所示（单位为比特）。 

![伯努利分布熵与概率的关系][3]  

当p=0或者p=1时，H(p)=0，随机变量完全没有不确定性。当p=0.5时，H(p)=1，熵取值最大，随机变量不确定性最大。  
设有随机变量(X,Y)，其联合概率分布为  

$$P(X=x_i,Y=y_i)=p_{ij},    i=1,2,...,n;  j=1,2,...,m$$ 

条件熵$$H(Y\vert X)$$表示在已知随机变量X的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵$$H(Y\vert X)$$，定义为X给定条件下Y的条件概率分布的熵对X的数学期望  

$$H(Y\vert X)=\sum_{i=1}^n p_i H(Y\vert X=x_i)$$ 

这里，$$p_i=P(X=x_i), i=1,2,...,n$$。  
当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵分别称为经验熵(empirical entropy)和经验条件熵(empirical conditional entropy)。此时，如果有0概率，令$$0\log 0=0$$。  
信息增益(information gain)表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。    

**定义** (**信息增益**)特征A对训练数据集D的信息增益$$g(D,A)$$，定义为集合D的经验熵$$H(D)$$与给定特征A条件下D的经验条件熵$$H(D\vert A)$$之差，即

$$g(D,A)=H(D)-H(D\vert A)$$

一般地，熵$$H(D)$$和条件熵$$H(D\vert A)$$之差称为互信息(mutual information)，决策树学习中的信息增益等价于训练数据集中类与特征的互信息。  
决策树学习应用信息增益准则选择特征。给定训练集D和特征A，经验熵$$H(D)$$表示对数据集D进行分类的不确定性。而经验条件熵$$H(D\vert A)$$表示在特征A给定的条件下对数据集D进行分类的不确定性。那么它们的差，即信息增益，就表示由于特征A而使得对数据集D的分类的不确定性减少的程度。显然，对数据集D而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益，信息增益大的特征具有更强的分类能力。  
根据信息增益准则的特征选择方法是：对训练数据集（或子集）D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。  
设训练数据集为D，\|D\|表示其样本容量，即样本个数。设有K个类C<sub>k</sub>，k=1,2,...,K，\|C<sub>k</sub>\|为属于类C<sub>k</sub>的样本个数，$$\sum_{k=1}^K \vert C_k \vert =\vert D \vert$$，设特征A有n个不同的取值$$\{a_1,a_2,...,a_n\}$$，根据特征A的取值将D划分为n个子集$$D_1,D_2,...,D_n$$，$$\vert D_i \vert$$为$$D_i$$的样本个数，$$\sum_{i=1}^n\vert D_i\vert =\vert D\vert$$。记子集$$D_i$$中属于类$$C_k$$的样本的集合为$$D_{ik}$$，即$$D_{ik}=D_i\bigcap C_k$$，$$\vert D_{ik} \vert$$为$$D_{ik}$$的样本个数。于是信息增益的算法如下：  
信息增益的算法：  
- 输入：训练数据集D和特征A；
- 输出：特征A对训练数据集D的信息增益g(D,A)。

1. 计算数据集D的经验熵H(D)  
\$\$ H(D)=-\sum_{k=1}^K \frac{\vert C_k\vert}{\vert D\vert}\log_2 \frac{\vert C_k\vert}{\vert D\vert} \$\$
1. 计算特征A对数据集D的经验条件熵H(D\|A)  
\$\$ H(D\vert A)=\sum_{i=1}^n \frac{\vert D_i\vert}{\vert D\vert}H(D_i)=-\sum_{i=1}^n \frac{\vert D_i\vert}{\vert D\vert} \sum_{k=1}^K \frac{\vert D_{ik}\vert}{\vert D_i\vert} \log_2 \frac{\vert D_{ik}\vert}{\vert D_i\vert} \$\$
1. 计算信息增益  
\$\$ g(D,A)=H(D)-H(D\vert A) \$\$

> 例子  &nbsp &nbsp 根据表格给的数据，利用信息增益准则选择最优特征。

解：首先计算经验熵H(D)。
\$\$H(D)=-\frac{9}{15}\log_2 \frac{9}{15}-\frac{6}{15}\log_2 \frac{6}{15}=0.971\$\$
然后计算各特征对数据集D的信息增益。分别以$$A_1,A_2,A_3,A_4$$表示年龄、有工作、有自己的房子和信贷情况4个特征，则

1. $$D_1,D_2,D_3$$指的$$A_1$$（年龄）取值为青年、中年和老年的样本子集。    
\$\$ \begin{align}
g(D\vert A_1) & = H(D)-[\frac{5}{15}H(D_1)+\frac{5}{15}H(D_2)+\frac{5}{15}H(D_3)] \\\\\\
& = 0.971-[\frac{5}{15}(-\frac{2}{5}\log_2 \frac{2}{5}-\frac{3}{5}\log_2 \frac{3}{5}) \\\\\\
& +\frac{5}{15}(-\frac{3}{5}\log_2 \frac{3}{5}-\frac{2}{5}\log_2 \frac{2}{5}) \\\\\\
& +\frac{5}{15}(-\frac{4}{5}\log_2 \frac{4}{5}--\frac{1}{5}\log_2 \frac{1}{5}]  \\\\\\
& = 0.971-0.888 \\\\\\
& = 0.083
\end{align} \$\$  
1. $$D_1,D_2$$对应有无工作的样本子集。  
\$\$
\begin{align}
g(D,A_2) & = H(D)-[\frac{5}{15}H(D_1)+\frac{10}{15}H(D_2)] \\\\\\
& = 0.971-[\frac{5}{15}\times 0+\frac{10}{15}(-\frac{4}{10}\log_2 \frac{4}{10}-\frac{6}{10}\log_2 \frac{6}{10})] \\\\\\
& = 0.324
\end{align}
\$\$
1. 同理可得  
\$\$
\begin{align}
g(D,A_3) & = 0.971-[\frac{6}{15}\times 0+\frac{9}{15}(-\frac{3}{9}\log_2 \frac{3}{9}-\frac{6}{9}\log_2 {6}{9})] \\\\\\
& = 0.420
\end{align}
\$\$
1. 同理可得  
\$\$
\begin{align}
g(D,A_4) & = 0.971-0.608 \\\\\\
& = 0.363
\end{align}
\$\$

最后比较各特征的信息增益值，由于特征$$A_3$$（有自己的房子）的信息增益值最大，所以选择特征$$A_3$$作为最优特征。

### 信息增益比

信息增益值的大小是相对于训练数据集而言的，并没有绝对意义。在分类问题困难时，也就是说训练数据集的熵大的时候，信息增益值会偏大。反之，信息增益值会偏小。使用信息增益比(information gain ratio)可以对这一问题进行校正，这是特征选择的另一准则。  
**定义(信息增益比) &nbsp**特征A对训练数据集的信息增益比$$g_R(D,A)$$定义为其信息增益$$g(D,A)$$与训练数据集D的经验熵$$H(D)$$之比：  

$$g_R(D,A)=\frac{g(D,A)}{H(D)}$$



### ID3算法
### C4.5的生成算法

## 决策树的剪枝

## CART算法
### CART生成
### CART剪枝

## 本文概要

参考李航的《统计学习方法》


[1]: /assets/images/decision_tree.png "决策树"
[2]: /assets/images/maybe_learn.png
[3]: /assets/images/b_prob.png
